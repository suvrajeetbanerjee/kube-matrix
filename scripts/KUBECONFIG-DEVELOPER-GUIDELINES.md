# **1️⃣ Kubeconfig Generation Script for Developers**

Save the following as `generate-kubeconfig.sh`:

**\#\!/bin/bash**  
**\# Script to generate kubeconfig for a developer for an existing EKS cluster**

**\# \-------------------------**  
**\# Configurable parameters**  
**\# \-------------------------**  
**CLUSTER\_NAME=${1:-"\<your-cluster-name\>"}     \# Default: replace with your cluster name**  
**REGION=${2:-"ap-south-1"}                   \# Default: Mumbai**  
**KUBECONFIG\_FILE=${3:-"$HOME/.kube/config"}  \# Default kubeconfig location**

**\# \-------------------------**  
**\# Prerequisites check**  
**\# \-------------------------**  
**command \-v aws \>/dev/null 2\>&1 || { echo \>&2 "AWS CLI not installed. Exiting."; exit 1; }**  
**command \-v kubectl \>/dev/null 2\>&1 || { echo \>&2 "kubectl not installed. Exiting."; exit 1; }**

**\# \-------------------------**  
**\# Generate kubeconfig**  
**\# \-------------------------**  
**echo "Generating kubeconfig for EKS cluster '$CLUSTER\_NAME' in region '$REGION'..."**  
**aws eks \--region $REGION update-kubeconfig \--name $CLUSTER\_NAME \--kubeconfig $KUBECONFIG\_FILE**

**\# \-------------------------**  
**\# Test connection**  
**\# \-------------------------**  
**echo "Testing connection..."**  
**kubectl get nodes**

**if \[ $? \-eq 0 \]; then**  
    **echo "✅ Kubeconfig setup successful. You can now run kubectl commands."**  
**else**  
    **echo "❌ Failed to connect. Check your AWS credentials and cluster permissions."**  
**fi**

**Usage Example:**
**bash generate-kubeconfig.sh <eks-cluster-name> <region>**
**bash generate-kubeconfig.sh dev-km-eks us-east-1**

This will create/update the kubeconfig at `~/.kube/config` and test connectivity.

---

# **2️⃣ Developer Guidelines for Sanity Testing**

### **Access Rules**

* Only use the kubeconfig generated by the script.

* Access should be **read-only or limited namespace** if possible (via RBAC).

* Do not perform Terraform applies on production cluster.

### **Namespace for Testing**

* Use a **dedicated namespace** for dev sanity testing, e.g., `dev-sanity`:

kubectl create namespace dev-sanity

* All test pods/services should be deployed in this namespace.

---

### **Deploying Test Pods**

Example:

**apiVersion: v1**  
**kind: Pod**  
**metadata:**  
  **name: nginx-test**  
  **namespace: dev-sanity**  
  **labels:**  
    **app: nginx-test**  
**spec:**  
  **containers:**  
  **\- name: nginx**  
    **image: nginx:latest**  
    **ports:**  
    **\- containerPort: 80**

**kubectl apply \-f nginx-test.yaml**  
**kubectl get pods \-n dev-sanity**

---

### **Exposing Test Pods**

Make sure pods have **labels**:

**kubectl expose pod nginx-test \\**  
  **\--type=ClusterIP \\**  
  **\--port=80 \\**  
  **\--target-port=80 \\**  
  **\--namespace=dev-sanity \\**  
  **\--name=nginx-test-svc**  
**kubectl get svc \-n dev-sanity**

---

### **Clean-Up After Testing**

**kubectl delete namespace dev-sanity**

* This removes all test pods/services safely.

* Leaves the production cluster untouched.

---

### 

### **Best Practices**

1. Always **run `kubectl get pods -A` or `kubectl get ns`** first to confirm what’s running.

2. Avoid creating resources in **default namespace**.

3. Use **labels** for all test pods to avoid conflicts.

4. Do **not delete or scale production workloads**.

